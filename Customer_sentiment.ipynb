{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Use case-3 : Predict Customer Sentiment\n",
    "\n",
    "# Problem statement --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the current world, there is no automatic system in place to find whether customer's feedback is positive or negative or neutral. \n",
    "# Build a classification model to decide whether it is a positive or negative or neutral category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Type of ML problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ====== It is a multi class classification problem.\n",
    " \n",
    " Multiclass classification is a classification task with more than two classes. Each sample can only be labeled as one class.\n",
    " \n",
    " __Credit__: http://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "In this problem, each of the sample can take only 1 label - positive OR negative OR neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use F1 score. we will try to build a model with higher F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory data analysis\n",
    "# Assumption - we are doing analysis across airlines. We are not focussing on any specific airline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.1 Data loading ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalData = pd.read_csv('Usecase3_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline  \\\n",
       "0           neutral  Virgin America   \n",
       "1          positive  Virgin America   \n",
       "2           neutral  Virgin America   \n",
       "3          negative  Virgin America   \n",
       "4          negative  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting CSV data into .db as its easy to query data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.2 Check for missing values ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   airline_sentiment  14640 non-null  object\n",
      " 1   airline            14640 non-null  object\n",
      " 2   text               14640 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 343.2+ KB\n"
     ]
    }
   ],
   "source": [
    "originalData.info()\n",
    "# we could see that there are no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== 3.3 Check for duplicate data ====\n",
    "\n",
    "it is necessary to check for duplicates in order to get unbiased results for the analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting CSV data into .db as its easy to query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalData.to_sql('data', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time taken to find duplicate records  0:00:00.077419\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "dupRecords = pd.read_sql_query(\"\"\" SELECT airline_sentiment,airline,text,COUNT(*) AS cnt_duplicates FROM data GROUP BY\n",
    "airline_sentiment,airline,text \"\"\", conn)\n",
    "print(\"total time taken to find duplicate records \", datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>cnt_duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>#nothelpful MT @AmericanAir: Our call volume i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir (2/3) ...I sat on the runway for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @C2Next Would be great to get so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @Kaha58 Would be nice if you cou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @USAirways Add insult to injury ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment   airline  \\\n",
       "0          negative  American   \n",
       "1          negative  American   \n",
       "2          negative  American   \n",
       "3          negative  American   \n",
       "4          negative  American   \n",
       "\n",
       "                                                text  cnt_duplicates  \n",
       "0  #nothelpful MT @AmericanAir: Our call volume i...               1  \n",
       "1  .@AmericanAir (2/3) ...I sat on the runway for...               1  \n",
       "2  .@AmericanAir @C2Next Would be great to get so...               1  \n",
       "3  .@AmericanAir @Kaha58 Would be nice if you cou...               1  \n",
       "4  .@AmericanAir @USAirways Add insult to injury ...               1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupRecords.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14286\n",
       "2      154\n",
       "3        6\n",
       "5        4\n",
       "4        2\n",
       "Name: cnt_duplicates, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each record appeared in our database\n",
    "dupRecords['cnt_duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14452 entries, 0 to 14451\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   airline_sentiment  14452 non-null  object\n",
      " 1   airline            14452 non-null  object\n",
      " 2   text               14452 non-null  object\n",
      " 3   cnt_duplicates     14452 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# find no of duplicate records.\n",
    "dupRecords.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " total no of duplicate records  188\n",
      " % of duplicate records  1.28\n"
     ]
    }
   ],
   "source": [
    "# total no of OR % of duplicate records.\n",
    "print(\" total no of duplicate records \", 14640 - 14452)\n",
    "print(\" % of duplicate records \", round((14640 - 14452)/14640*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we need to remove duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new database with no duplicates.\n",
    "if not os.path.isfile('data_no_dup.db'):\n",
    "    conn1 = sqlite3.connect('data_no_dup.db')\n",
    "    no_dup = pd.DataFrame(dupRecords, columns=['airline_sentiment', 'airline', 'text'])\n",
    "    no_dup.to_sql('data_no_dup',conn1)\n",
    "    conn1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2 = sqlite3.connect('data_no_dup.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup = pd.read_sql_query(\"\"\" SELECT * FROM data_no_dup\"\"\", conn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>#nothelpful MT @AmericanAir: Our call volume i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir (2/3) ...I sat on the runway for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @C2Next Would be great to get so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @Kaha58 Would be nice if you cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @USAirways Add insult to injury ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index airline_sentiment   airline  \\\n",
       "0      0          negative  American   \n",
       "1      1          negative  American   \n",
       "2      2          negative  American   \n",
       "3      3          negative  American   \n",
       "4      4          negative  American   \n",
       "\n",
       "                                                text  \n",
       "0  #nothelpful MT @AmericanAir: Our call volume i...  \n",
       "1  .@AmericanAir (2/3) ...I sat on the runway for...  \n",
       "2  .@AmericanAir @C2Next Would be great to get so...  \n",
       "3  .@AmericanAir @Kaha58 Would be nice if you cou...  \n",
       "4  .@AmericanAir @USAirways Add insult to injury ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>#nothelpful MT @AmericanAir: Our call volume i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir (2/3) ...I sat on the runway for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @C2Next Would be great to get so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @Kaha58 Would be nice if you cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>.@AmericanAir @USAirways Add insult to injury ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Awesome! RT @VirginAmerica: Watch nominated fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14448</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14449</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Nice RT @VirginAmerica: The man of steel might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14450</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>‚Äú@VirginAmerica: @KarinSLee Of course. Have fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14451</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>üòé RT @VirginAmerica: You‚Äôve met your match. Go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14452 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment         airline  \\\n",
       "0              negative        American   \n",
       "1              negative        American   \n",
       "2              negative        American   \n",
       "3              negative        American   \n",
       "4              negative        American   \n",
       "...                 ...             ...   \n",
       "14447          positive  Virgin America   \n",
       "14448          positive  Virgin America   \n",
       "14449          positive  Virgin America   \n",
       "14450          positive  Virgin America   \n",
       "14451          positive  Virgin America   \n",
       "\n",
       "                                                    text  \n",
       "0      #nothelpful MT @AmericanAir: Our call volume i...  \n",
       "1      .@AmericanAir (2/3) ...I sat on the runway for...  \n",
       "2      .@AmericanAir @C2Next Would be great to get so...  \n",
       "3      .@AmericanAir @Kaha58 Would be nice if you cou...  \n",
       "4      .@AmericanAir @USAirways Add insult to injury ...  \n",
       "...                                                  ...  \n",
       "14447  Awesome! RT @VirginAmerica: Watch nominated fi...  \n",
       "14448                    I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç  \n",
       "14449  Nice RT @VirginAmerica: The man of steel might...  \n",
       "14450  ‚Äú@VirginAmerica: @KarinSLee Of course. Have fu...  \n",
       "14451  üòé RT @VirginAmerica: You‚Äôve met your match. Go...  \n",
       "\n",
       "[14452 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Lets check for number of occurences of each of sentiment in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9087\n",
       "neutral     3067\n",
       "positive    2298\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a26232c50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='airline_sentiment',data=df_no_dup)\n",
    "# we could see that close to 63% of sentiment is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3.5 - Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check some random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_dup['text'].values[0])\n",
    "print(\"=\"*100)\n",
    "print(df_no_dup['text'].values[15])\n",
    "print(\"=\"*100)\n",
    "print(df_no_dup['text'].values[100])\n",
    "print(\"=\"*100)\n",
    "print(df_no_dup['text'].values[999])\n",
    "print(\"=\"*100)\n",
    "print(df_no_dup['text'].values[11000])\n",
    "print(\"=\"*100)\n",
    "print(df_no_dup['text'].values[12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Observation -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could see that there are a lot of punctuations, web site names, emoticons etc. we need to remove all of these before applying any of the text to vector featurization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.1 - convert text into lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTexts = df_no_dup['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLowerCase(col):\n",
    "    text = col[0]\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(convertToLowerCase,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.2 - Remove any URLs exist in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUrls(col):\n",
    "    text = col[0]\n",
    "    newText = re.sub(r\"http\\S+\", '', text)\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(removeUrls,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@americanair ... do i have to even say anything?  over 6 hours on hold... no one has ever sucked as much as you. \n"
     ]
    }
   ],
   "source": [
    "# check if unwanted URLs are removed.\n",
    "print(df_no_dup['text'].values[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.3 - Remove any HTML tags if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHtmlTags(col):\n",
    "    text = col[0]\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    newText = soup.get_text()\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(removeHtmlTags,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.4 - Expand english language contractions in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generic function which will return expanded phrases.\n",
    "def decontracted(col):\n",
    "    phrase = col[0]\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(decontracted,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.5 - Remove alphanumeric text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAlphaNumericText(col):\n",
    "    text = col[0]\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(removeAlphaNumericText,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.6 - Remove punctuations and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialChar(col):\n",
    "    text = col[0]\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(removeSpecialChar,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== 3.5.7 - Remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"won't\", 'by', \"doesn't\", 'any', 'mustn', 'not', \"you're\", \"don't\", 'll', 'then', 'were', 'them', 'haven', 'itself', \"hasn't\", 'and', 'until', 'of', 'below', 'shouldn', 'm', 'they', 'wouldn', 'don', 'from', 'up', 'only', 'further', 'which', 'aren', 'ours', 'or', 'isn', 'into', 'been', \"you've\", \"shan't\", 'am', 'at', 'i', 'was', \"shouldn't\", 'off', 'needn', \"wouldn't\", 'once', 'doing', 'these', 'to', \"you'd\", 'no', 'an', 'ain', 'mightn', 'ma', 'some', \"weren't\", 've', 'will', 'won', \"didn't\", 'have', 'few', 'most', 'are', 'our', \"needn't\", 'shan', \"couldn't\", 'your', 'more', \"hadn't\", 'about', 'ourselves', 'against', 'own', 'wasn', 'what', 'out', 'for', 'if', 'with', 'o', 'down', 'herself', 'above', 'but', 'under', \"you'll\", 'nor', 'there', 'here', 'too', 'now', 'is', 't', 'yourselves', 'be', \"aren't\", 's', 'before', 'himself', 're', 'as', 'we', 'its', 'how', 'being', 'same', \"should've\", 'hasn', 'so', 'than', 'myself', 'did', 'me', 'didn', 'he', 'where', 'doesn', 'you', 'him', 'her', 'his', 'during', \"wasn't\", 'a', 'all', 'because', 'she', 'each', 'after', 'couldn', 'the', 'those', 'hers', 'hadn', 'other', \"haven't\", 'themselves', 'again', 'such', 'on', 'can', 'y', 'has', \"that'll\", 'why', 'whom', 'who', 'it', 'having', 'in', 'through', 'very', 'their', 'theirs', \"mightn't\", 'between', 'just', 'yours', 'that', \"isn't\", 'this', 'when', 'my', 'weren', 'does', 'while', \"it's\", 'had', 'both', 'd', 'over', 'yourself', \"she's\", 'do', \"mustn't\", 'should'}\n"
     ]
    }
   ],
   "source": [
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have words like 'not' 'nor' 'no' in stopwords.\n",
    "# lets remove these words.\n",
    "stopWords.remove(\"not\")\n",
    "stopWords.remove(\"no\")\n",
    "stopWords.remove(\"nor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.add('br')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a generic function to remove stopwords.\n",
    "def removeStopwords(col):\n",
    "    sentence = col[0]\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopWords)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup['text'] = df_no_dup[['text']].apply(removeStopwords,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>nothelpful mt americanair call volume extremel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair sat runway hrs takeoff hrs late fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would great get help trying since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would nice could actually talk res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair usairways add insult injury guys m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index airline_sentiment   airline  \\\n",
       "0      0          negative  American   \n",
       "1      1          negative  American   \n",
       "2      2          negative  American   \n",
       "3      3          negative  American   \n",
       "4      4          negative  American   \n",
       "\n",
       "                                                text  \n",
       "0  nothelpful mt americanair call volume extremel...  \n",
       "1  americanair sat runway hrs takeoff hrs late fl...  \n",
       "2  americanair would great get help trying since ...  \n",
       "3  americanair would nice could actually talk res...  \n",
       "4  americanair usairways add insult injury guys m...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store intermediate processed data into Sqlite file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('processedData.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "df_no_dup.to_sql('processedData', conn, schema=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>nothelpful mt americanair call volume extremel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair sat runway hrs takeoff hrs late fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would great get help trying since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would nice could actually talk res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair usairways add insult injury guys m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index airline_sentiment   airline  \\\n",
       "0      0          negative  American   \n",
       "1      1          negative  American   \n",
       "2      2          negative  American   \n",
       "3      3          negative  American   \n",
       "4      4          negative  American   \n",
       "\n",
       "                                                text  \n",
       "0  nothelpful mt americanair call volume extremel...  \n",
       "1  americanair sat runway hrs takeoff hrs late fl...  \n",
       "2  americanair would great get help trying since ...  \n",
       "3  americanair would nice could actually talk res...  \n",
       "4  americanair usairways add insult injury guys m...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Label encoding of airline_sentiment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning numerical values and storing in another column\n",
    "df_no_dup['airline_sentiment_encoded'] = labelencoder.fit_transform(df_no_dup['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9087\n",
       "1    3067\n",
       "2    2298\n",
       "Name: airline_sentiment_encoded, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup['airline_sentiment_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>nothelpful mt americanair call volume extremel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair sat runway hrs takeoff hrs late fl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would great get help trying since ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair would nice could actually talk res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>americanair usairways add insult injury guys m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index airline_sentiment   airline  \\\n",
       "0      0          negative  American   \n",
       "1      1          negative  American   \n",
       "2      2          negative  American   \n",
       "3      3          negative  American   \n",
       "4      4          negative  American   \n",
       "\n",
       "                                                text  \\\n",
       "0  nothelpful mt americanair call volume extremel...   \n",
       "1  americanair sat runway hrs takeoff hrs late fl...   \n",
       "2  americanair would great get help trying since ...   \n",
       "3  americanair would nice could actually talk res...   \n",
       "4  americanair usairways add insult injury guys m...   \n",
       "\n",
       "   airline_sentiment_encoded  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_no_dup['airline_sentiment_encoded'].values\n",
    "X = df_no_dup['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cross validation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33) # this is random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9682,) (9682,)\n",
      "(4770,) (4770,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply BOW to convert text into numerical vectors.\n",
    "\n",
    "# we could also use TFIDF, Word2Vec and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train) # fit has to happen only on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Checking vector dimenions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(9682, 9392) (9682,)\n",
      "(4770, 9392) (4770,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"After vectorizations\")\n",
    "print(X_train_bow.shape, y_train.shape)\n",
    "print(X_test_bow.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "# we get around 9k dimensions vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes ==\n",
    "\n",
    "# As Naive Bayes is considered as a benchmark ML algorithm for text classification, we will use it. \n",
    "\n",
    "# we could also use different classifiers and compare model performance to come up with final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With default hyperparameter 1.0\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculate f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7622641509433963\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_pred, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1.0,1.1,1.2,1.3]\n",
    "max_f1_score = float('-inf')\n",
    "best_alpha = None\n",
    "for alpha in alpha_values:\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    y_pred = clf.predict(X_test_bow)\n",
    "    current_f1_score = f1_score(y_pred, y_test, average='micro')\n",
    "    if current_f1_score > max_f1_score:\n",
    "        max_f1_score = current_f1_score\n",
    "        best_alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max f1 score is ==  0.7714884696016772\n",
      "best alpha is ==  0.1\n"
     ]
    }
   ],
   "source": [
    "print('max f1 score is == ', max_f1_score)\n",
    "print('best alpha is == ', best_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
